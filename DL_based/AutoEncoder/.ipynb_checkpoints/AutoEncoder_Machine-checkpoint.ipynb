{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = '/Users/JH/Desktop/NTU/NTU_Research/data/NEM_Load_Forecasting_Database.xls'\n",
    "\n",
    "QLD = 'Actual_Data_QLD'\n",
    "NSW = 'Actual_Data_NSW'\n",
    "VIC = 'Actual_Data_VIC'\n",
    "SA = 'Actual_Data_SA'\n",
    "TAS = 'Actual_Data_TAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Classes as Data Container\n",
    "\n",
    "class Structure:\n",
    "    def __init__(self):\n",
    "        self._feature = []\n",
    "        self._target = []\n",
    "\n",
    "    @property\n",
    "    def feature(self):\n",
    "        return self._feature\n",
    "\n",
    "    @property\n",
    "    def target(self):\n",
    "        return self._target\n",
    "\n",
    "    @feature.setter\n",
    "    def feature(self, value):\n",
    "        self._feature = value\n",
    "\n",
    "    @target.setter\n",
    "    def target(self, value):\n",
    "        self._target = value\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    class Train(Structure):\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "    class Test(Structure):\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    class Raw:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        class Train(Structure):\n",
    "            def __init__(self):\n",
    "                pass\n",
    "\n",
    "        class Test(Structure):\n",
    "            def __init__(self):\n",
    "                pass\n",
    "\n",
    "    class PreProcessed:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        class Train(Structure):\n",
    "            def __init__(self):\n",
    "                pass\n",
    "\n",
    "        class Test(Structure):\n",
    "            def __init__(self):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Functions\n",
    "\n",
    "def normalization(data):\n",
    "    return (data - min(data)) / (max(data) - min(data))\n",
    "\n",
    "\n",
    "def data_splitter(data, ratio=0.8):\n",
    "    \"\"\"\n",
    "    split data into training data & testing data\n",
    "    :param data:\n",
    "\n",
    "    :param ratio:\n",
    "        training data ratio\n",
    "    :return:\n",
    "        train_data, test_data\n",
    "    \"\"\"\n",
    "    splitter = int(len(data) * ratio)\n",
    "    return np.array(data[:splitter]), np.array(data[splitter + 1:])\n",
    "\n",
    "\n",
    "def preprocessing_filter(data, nominator, denominator):\n",
    "    return normalization(data) ** (nominator / denominator)\n",
    "\n",
    "\n",
    "def preprocessing(data_present, temperature_max, temperature_mean, denominator):\n",
    "    data_present = list(data_present) + list(\n",
    "        preprocessing_filter(np.array(data_present), temperature_max, denominator)) + list(\n",
    "        preprocessing_filter(np.array(data_present), temperature_mean, denominator))\n",
    "\n",
    "    return np.array(data_present)\n",
    "\n",
    "\n",
    "def data_alloter(df):\n",
    "    dataset = DataSet()\n",
    "    denominator = df['Mean Tem.'].min()\n",
    "\n",
    "    raw_feature = []\n",
    "    raw_target = []\n",
    "    preprocessed_feature = []\n",
    "    preprocessed_target = []\n",
    "\n",
    "    for row in range(0, len(df)):\n",
    "        # if both MaxTemp and MeanTemp are not nan\n",
    "        if not math.isnan(df['Max Tem.'][row]) and not math.isnan(df['Mean Tem.'][row]):\n",
    "            if not math.isnan(df['Max Tem.'][row + 1]) and not math.isnan(df['Mean Tem.'][row + 1]):\n",
    "                powerload_present = normalization(np.array(df.loc[row][5:53]))\n",
    "                powerload_future = normalization(np.array(df.loc[row + 1][5:53]))\n",
    "\n",
    "                raw_feature.append(np.array(\n",
    "                    list(powerload_present) + list([df['Max Tem.'][row + 1]]) + list([df['Mean Tem.'][row + 1]])))\n",
    "                raw_target.append(np.array(powerload_future))\n",
    "\n",
    "                preprocessed_powerload_present = preprocessing(powerload_present,\n",
    "                                                               df['Max Tem.'][row + 1],\n",
    "                                                               df['Mean Tem.'][row + 1],\n",
    "                                                               denominator)\n",
    "\n",
    "                preprocessed_feature.append(preprocessed_powerload_present)\n",
    "                preprocessed_target.append(np.array(powerload_future))\n",
    "\n",
    "    dataset.Raw.Train.feature, dataset.Raw.Test.feature = data_splitter(raw_feature)\n",
    "    dataset.Raw.Train.target, dataset.Raw.Test.target = data_splitter(raw_target)\n",
    "\n",
    "    dataset.PreProcessed.Train.feature, dataset.PreProcessed.Test.feature = data_splitter(preprocessed_feature)\n",
    "    dataset.PreProcessed.Train.target, dataset.PreProcessed.Test.target = data_splitter(preprocessed_target)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(file, sheetname=QLD)\n",
    "dataset = data_alloter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 100\n",
    "num_steps = 4000\n",
    "data_showing_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 144 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 48 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "learning_rate_decayed = tf.placeholder(tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_decayed).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0100 cost= 20987.307291667\n",
      "step: 0200 cost= 1807950.500000000\n",
      "step: 0300 cost= 5200889.333333333\n",
      "step: 0400 cost= 8340340.666666667\n",
      "step: 0500 cost= 10821931.333333334\n",
      "step: 0600 cost= 21421960.000000000\n",
      "step: 0700 cost= 32708048.000000000\n",
      "step: 0800 cost= 37048056.000000000\n",
      "step: 0900 cost= 59911461.333333336\n",
      "step: 1000 cost= 100745301.333333328\n",
      "step: 1100 cost= 90215946.666666672\n",
      "step: 1200 cost= 134468256.000000000\n",
      "step: 1300 cost= 192034645.333333343\n",
      "step: 1400 cost= 147097760.000000000\n",
      "step: 1500 cost= 215002944.000000000\n",
      "step: 1600 cost= 272006080.000000000\n",
      "step: 1700 cost= 238145258.666666657\n",
      "step: 1800 cost= 297213781.333333313\n",
      "step: 1900 cost= 396737962.666666687\n",
      "step: 2000 cost= 302026880.000000000\n",
      "step: 2100 cost= 415295146.666666687\n",
      "step: 2200 cost= 498045952.000000000\n",
      "step: 2300 cost= 387208704.000000000\n",
      "step: 2400 cost= 493296298.666666687\n",
      "step: 2500 cost= 593644928.000000000\n",
      "step: 2600 cost= 467237930.666666687\n",
      "step: 2700 cost= 522500906.666666687\n",
      "step: 2800 cost= 734245632.000000000\n",
      "step: 2900 cost= 572816213.333333373\n",
      "step: 3000 cost= 686987434.666666627\n",
      "step: 3100 cost= 854486613.333333373\n",
      "step: 3200 cost= 680494592.000000000\n",
      "step: 3300 cost= 775745962.666666627\n",
      "step: 3400 cost= 958057642.666666627\n",
      "step: 3500 cost= 712289962.666666627\n",
      "step: 3600 cost= 928127744.000000000\n",
      "step: 3700 cost= 1110528853.333333254\n",
      "step: 3800 cost= 832140800.000000000\n",
      "step: 3900 cost= 985938261.333333373\n",
      "step: 4000 cost= 1244429141.333333254\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "# Training cycle\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    avg_cost = 0.\n",
    "    total_batch = int(dataset.PreProcessed.Train.feature.shape[0]/batch_size)\n",
    "    \n",
    "    if 30000 < step < 80000:\n",
    "        learning_rate = 0.01 / 10\n",
    "    elif 80000 <= step < 200000:\n",
    "        learning_rate = 0.01 / 100\n",
    "    else:\n",
    "        learning_rate = 0.01\n",
    "\n",
    "    # set a offset\n",
    "    offset = (step * batch_size) % (dataset.PreProcessed.Train.target.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_x = dataset.PreProcessed.Train.feature[offset:(offset + batch_size), :]\n",
    "    batch_y = dataset.PreProcessed.Train.target[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {x: batch_x, \n",
    "                 y: batch_y,\n",
    "                 learning_rate_decayed: learning_rate}\n",
    "    \n",
    "    _, c = sess.run([optimizer, cost], feed_dict=feed_dict)\n",
    "    \n",
    "    # Compute average loss\n",
    "    avg_cost += c / total_batch\n",
    "    \n",
    "    if (step % data_showing_step == 0):\n",
    "        print \"step:\", '%04d' % (step+batch_size), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
