{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = '/Users/JH/Desktop/NTU/NTU_Research/data/NEM_Load_Forecasting_Database.xls'\n",
    "\n",
    "QLD = 'Actual_Data_QLD'\n",
    "NSW = 'Actual_Data_NSW'\n",
    "VIC = 'Actual_Data_VIC'\n",
    "SA = 'Actual_Data_SA'\n",
    "TAS = 'Actual_Data_TAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Classes as Data Container\n",
    "\n",
    "class Structure:\n",
    "    def __init__(self):\n",
    "        self._feature = []\n",
    "        self._target = []\n",
    "\n",
    "    @property\n",
    "    def feature(self):\n",
    "        return self._feature\n",
    "\n",
    "    @property\n",
    "    def target(self):\n",
    "        return self._target\n",
    "\n",
    "    @feature.setter\n",
    "    def feature(self, value):\n",
    "        self._feature = value\n",
    "\n",
    "    @target.setter\n",
    "    def target(self, value):\n",
    "        self._target = value\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    class Train(Structure):\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "    class Test(Structure):\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    class Raw:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        class Train(Structure):\n",
    "            def __init__(self):\n",
    "                pass\n",
    "\n",
    "        class Test(Structure):\n",
    "            def __init__(self):\n",
    "                pass\n",
    "\n",
    "    class PreProcessed:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        class Train(Structure):\n",
    "            def __init__(self):\n",
    "                pass\n",
    "\n",
    "        class Test(Structure):\n",
    "            def __init__(self):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Functions\n",
    "\n",
    "def normalization(data):\n",
    "    return (data - min(data)) / (max(data) - min(data))\n",
    "\n",
    "\n",
    "def data_splitter(data, ratio=0.8):\n",
    "    \"\"\"\n",
    "    split data into training data & testing data\n",
    "    :param data:\n",
    "\n",
    "    :param ratio:\n",
    "        training data ratio\n",
    "    :return:\n",
    "        train_data, test_data\n",
    "    \"\"\"\n",
    "    splitter = int(len(data) * ratio)\n",
    "    return np.array(data[:splitter]), np.array(data[splitter + 1:])\n",
    "\n",
    "\n",
    "def preprocessing_filter(data, nominator, denominator):\n",
    "    return normalization(data) ** (nominator / denominator)\n",
    "\n",
    "\n",
    "def preprocessing(data_present, temperature_max, temperature_mean, denominator):\n",
    "    data_present = list(data_present) + list(\n",
    "        preprocessing_filter(np.array(data_present), temperature_max, denominator)) + list(\n",
    "        preprocessing_filter(np.array(data_present), temperature_mean, denominator))\n",
    "\n",
    "    return np.array(data_present)\n",
    "\n",
    "\n",
    "def data_alloter(df):\n",
    "    dataset = DataSet()\n",
    "    denominator = df['Mean Tem.'].min()\n",
    "\n",
    "    raw_feature = []\n",
    "    raw_target = []\n",
    "    preprocessed_feature = []\n",
    "    preprocessed_target = []\n",
    "\n",
    "    for row in range(0, len(df)):\n",
    "        # if both MaxTemp and MeanTemp are not nan\n",
    "        if not math.isnan(df['Max Tem.'][row]) and not math.isnan(df['Mean Tem.'][row]):\n",
    "            if not math.isnan(df['Max Tem.'][row + 1]) and not math.isnan(df['Mean Tem.'][row + 1]):\n",
    "                powerload_present = normalization(np.array(df.loc[row][5:53]))\n",
    "                powerload_future = normalization(np.array(df.loc[row + 1][5:53]))\n",
    "\n",
    "                raw_feature.append(np.array(\n",
    "                    list(powerload_present) + list([df['Max Tem.'][row + 1]]) + list([df['Mean Tem.'][row + 1]])))\n",
    "                raw_target.append(np.array(powerload_future))\n",
    "\n",
    "                preprocessed_powerload_present = preprocessing(powerload_present,\n",
    "                                                               df['Max Tem.'][row + 1],\n",
    "                                                               df['Mean Tem.'][row + 1],\n",
    "                                                               denominator)\n",
    "\n",
    "                preprocessed_feature.append(preprocessed_powerload_present)\n",
    "                preprocessed_target.append(np.array(powerload_future))\n",
    "\n",
    "    dataset.Raw.Train.feature, dataset.Raw.Test.feature = data_splitter(raw_feature)\n",
    "    dataset.Raw.Train.target, dataset.Raw.Test.target = data_splitter(raw_target)\n",
    "\n",
    "    dataset.PreProcessed.Train.feature, dataset.PreProcessed.Test.feature = data_splitter(preprocessed_feature)\n",
    "    dataset.PreProcessed.Train.target, dataset.PreProcessed.Test.target = data_splitter(preprocessed_target)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(file, sheetname=QLD)\n",
    "dataset = data_alloter(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsed Data Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\"\"\"\n",
    "learning_rate = 0.01\n",
    "training_iters = 3000\n",
    "display_step = 100\n",
    "\"\"\"\n",
    "batch_size = 100\n",
    "num_steps = 3000\n",
    "data_showing_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 144  # feature data as input (input matrix shape: ???)\n",
    "n_output = 48  # target data as output  (48-points)\n",
    "dropout = 0.75  # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input], name='feature_input')\n",
    "y = tf.placeholder(tf.float32, [None, n_output], name='target_output')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')  # dropout (keep probability)\n",
    "learning_rate_decayed = tf.placeholder(tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    with tf.name_scope('array_reshape') as array_reshape:\n",
    "        x = tf.reshape(x, shape=[-1, 12, 12, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    with tf.name_scope('conv_layer1') as conv_layer1:\n",
    "        conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    with tf.name_scope('conv_layer2') as conv_layer2:\n",
    "        conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    with tf.name_scope('fc_layer') as fc_layer:\n",
    "        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        # Apply Dropout\n",
    "        fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    with tf.name_scope('output_layer') as output_layer:\n",
    "        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "        # out = tf.div(tf.subtract(out, tf.reduce_min(out)), tf.subtract(tf.reduce_max(out), tf.reduce_min(out)))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 3x3 conv, 1 input, 32 outputs(number of filter = 32)\n",
    "    'wc1': tf.Variable(tf.truncated_normal([3, 3, 1, 32]), name='wc1'),\n",
    "    \n",
    "    # 3x3 conv, 32 inputs, 64 outputs(number of filter = 64)\n",
    "    'wc2': tf.Variable(tf.truncated_normal([3, 3, 32, 64]), name='wc2'),\n",
    "    \n",
    "    # fully connected, width*height*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.truncated_normal([3 * 3 * 64, 64*9]), name='wd1'),\n",
    "    \n",
    "    # 1024 inputs, 48 outputs\n",
    "    'out': tf.Variable(tf.truncated_normal([64*9, n_output]), name='wo1')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.truncated_normal([32]), name='bc1'),\n",
    "    'bc2': tf.Variable(tf.truncated_normal([64]), name='bc2'),\n",
    "    'bd1': tf.Variable(tf.truncated_normal([64*9]), name='bd1'),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_output]), name='bo1')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope('cost'):\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    tf.summary.scalar('cost/', cost)\n",
    "    \n",
    "with tf.name_scope('optimization'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_decayed).minimize(cost)\n",
    "\n",
    "# Evaluation model\n",
    "with tf.name_scope('evaluation'):\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y, pred))))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0100 cost= 617.036539714\n",
      "step: 0200 cost= 6.554775238\n",
      "step: 0300 cost= 4.864187876\n",
      "step: 0400 cost= 5.290754000\n",
      "step: 0500 cost= 2.848626455\n",
      "step: 0600 cost= 2.924315135\n",
      "step: 0700 cost= 3.290879250\n",
      "step: 0800 cost= 2.328847090\n",
      "step: 0900 cost= 2.024056594\n",
      "step: 1000 cost= 2.375566959\n",
      "step: 1100 cost= 1.642885367\n",
      "step: 1200 cost= 1.149633010\n",
      "step: 1300 cost= 1.624999523\n",
      "step: 1400 cost= 1.025302092\n",
      "step: 1500 cost= 1.063786904\n",
      "step: 1600 cost= 1.536279678\n",
      "step: 1700 cost= 0.963502010\n",
      "step: 1800 cost= 1.002991835\n",
      "step: 1900 cost= 1.456308842\n",
      "step: 2000 cost= 0.907831510\n",
      "step: 2100 cost= 0.939590772\n",
      "step: 2200 cost= 1.400927385\n",
      "step: 2300 cost= 0.882047256\n",
      "step: 2400 cost= 0.928760052\n",
      "step: 2500 cost= 1.392327468\n",
      "step:"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "# Training cycle\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    avg_cost = 0.\n",
    "    total_batch = int(dataset.PreProcessed.Train.feature.shape[0]/batch_size)\n",
    "    \n",
    "    if 1000 < step < 2000:\n",
    "        learning_rate = 0.01 / 100\n",
    "    elif 2000 <= step < 5000:\n",
    "        learning_rate = 0.01 / 10000\n",
    "    else:\n",
    "        learning_rate = 0.01\n",
    "\n",
    "    # set a offset\n",
    "    offset = (step * batch_size) % (dataset.PreProcessed.Train.target.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    batch_x = dataset.PreProcessed.Train.feature[offset:(offset + batch_size), :]\n",
    "    batch_y = dataset.PreProcessed.Train.target[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, \n",
    "                                   keep_prob: 1., \n",
    "                                   learning_rate_decayed: learning_rate})\n",
    "    \n",
    "    c = sess.run(cost, feed_dict={x: batch_x, y: batch_y, \n",
    "                                  keep_prob: 1.})\n",
    "    \n",
    "    # Compute average loss\n",
    "    avg_cost += c / total_batch\n",
    "    \n",
    "    if (step % data_showing_step == 0):\n",
    "        print \"step:\", '%04d' % (step+batch_size), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "        \n",
    "print \"Optimization Finished!\"\n",
    "\n",
    "print \"Training Accuracy [RMSE] :\",\\\n",
    "    sess.run(rmse, feed_dict={x: dataset.PreProcessed.Test.feature, \n",
    "                              y: dataset.PreProcessed.Test.target,\n",
    "                              keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Testing Accuracy [RMSE] :\",\\\n",
    "    sess.run(rmse, feed_dict={x: dataset.PreProcessed.Test.feature, \n",
    "                              y: dataset.PreProcessed.Test.target, \n",
    "                              keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = sess.run(pred, feed_dict={x: dataset.PreProcessed.Test.feature,\n",
    "                                    y: dataset.PreProcessed.Test.target, \n",
    "                                    keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print predict[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dataset.PreProcessed.Test.target[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time = np.arange(0, 48, 1)\n",
    "\n",
    "plt.plot(time, normalization(predict[11]), 'r', \n",
    "         time, dataset.PreProcessed.Test.target[11], 'g')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
