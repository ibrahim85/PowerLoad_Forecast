{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = '/Users/JH/Desktop/NTU/NTU_Research/data/NEM_Load_Forecasting_Database.xls'\n",
    "\n",
    "concatenate_number = 2*4*4\n",
    "\n",
    "QLD = 'Actual_Data_QLD'\n",
    "NSW = 'Actual_Data_NSW'\n",
    "VIC = 'Actual_Data_VIC'\n",
    "SA = 'Actual_Data_SA'\n",
    "TAS = 'Actual_Data_TAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Classes as Data Container\n",
    "\n",
    "class Structure:\n",
    "    def __init__(self):\n",
    "        self._feature = []\n",
    "        self._target = []\n",
    "\n",
    "    @property\n",
    "    def feature(self):\n",
    "        return self._feature\n",
    "\n",
    "    @property\n",
    "    def target(self):\n",
    "        return self._target\n",
    "\n",
    "    @feature.setter\n",
    "    def feature(self, value):\n",
    "        self._feature = value\n",
    "\n",
    "    @target.setter\n",
    "    def target(self, value):\n",
    "        self._target = value\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    class Train(Structure):\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "    class Test(Structure):\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    class Raw:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        class Train(Structure):\n",
    "            def __init__(self):\n",
    "                pass\n",
    "\n",
    "        class Test(Structure):\n",
    "            def __init__(self):\n",
    "                pass\n",
    "\n",
    "    class PreProcessed:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        class Train(Structure):\n",
    "            def __init__(self):\n",
    "                pass\n",
    "\n",
    "        class Test(Structure):\n",
    "            def __init__(self):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Functions\n",
    "\n",
    "def normalization(data):\n",
    "    return (data - min(data)) / (max(data) - min(data))\n",
    "\n",
    "\n",
    "def data_splitter(data, ratio=0.8):\n",
    "    \"\"\"\n",
    "    split data into training data & testing data\n",
    "    :param data:\n",
    "\n",
    "    :param ratio:\n",
    "        training data ratio\n",
    "    :return:\n",
    "        train_data, test_data\n",
    "    \"\"\"\n",
    "    splitter = int(len(data) * ratio)\n",
    "    return np.array(data[:splitter]), np.array(data[splitter + 1:])\n",
    "\n",
    "\n",
    "def preprocessing_filter(data, nominator, denominator):\n",
    "    return normalization(data) ** (nominator / denominator)\n",
    "\n",
    "\n",
    "def preprocessing(data_present, temperature_max, temperature_mean, denominator):\n",
    "    data_present = list(data_present) + list(\n",
    "        preprocessing_filter(np.array(data_present), temperature_max, denominator)) + list(\n",
    "        preprocessing_filter(np.array(data_present), temperature_mean, denominator))\n",
    "\n",
    "    return np.array(data_present)\n",
    "\n",
    "\n",
    "def data_alloter(df):\n",
    "    dataset = DataSet()\n",
    "    denominator = df['Mean Tem.'].min()\n",
    "\n",
    "    raw_feature = []\n",
    "    raw_target = []\n",
    "    preprocessed_feature = []\n",
    "    preprocessed_target = []\n",
    "\n",
    "    for row in range(0, len(df)):\n",
    "        # if both MaxTemp and MeanTemp are not nan\n",
    "        if not math.isnan(df['Max Tem.'][row]) and not math.isnan(df['Mean Tem.'][row]):\n",
    "            if not math.isnan(df['Max Tem.'][row + 1]) and not math.isnan(df['Mean Tem.'][row + 1]):\n",
    "                powerload_present = normalization(np.array(df.loc[row][5:53]))\n",
    "                powerload_future = normalization(np.array(df.loc[row + 1][5:53]))\n",
    "\n",
    "                raw_feature.append(np.array(\n",
    "                    list(powerload_present) + list([df['Max Tem.'][row + 1]]) + list([df['Mean Tem.'][row + 1]])))\n",
    "                raw_target.append(np.array(powerload_future))\n",
    "\n",
    "                preprocessed_powerload_present = preprocessing(powerload_present,\n",
    "                                                               df['Max Tem.'][row + 1],\n",
    "                                                               df['Mean Tem.'][row + 1],\n",
    "                                                               denominator)\n",
    "\n",
    "                preprocessed_feature.append(preprocessed_powerload_present)\n",
    "                preprocessed_target.append(np.array(powerload_future))\n",
    "\n",
    "    dataset.Raw.Train.feature, dataset.Raw.Test.feature = data_splitter(raw_feature)\n",
    "    dataset.Raw.Train.target, dataset.Raw.Test.target = data_splitter(raw_target)\n",
    "\n",
    "    dataset.PreProcessed.Train.feature, dataset.PreProcessed.Test.feature = data_splitter(preprocessed_feature)\n",
    "    dataset.PreProcessed.Train.target, dataset.PreProcessed.Test.target = data_splitter(preprocessed_target)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(file, sheetname=QLD)\n",
    "dataset = data_alloter(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsed Data Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_iters = 200000\n",
    "    #batch_size = 128\n",
    "display_step = 50\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 144  # feature data as input (input matrix shape: ???)\n",
    "n_output = 48  # target data as output  (48-points)\n",
    "dropout = 0.75  # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input], name='feature_input')\n",
    "y = tf.placeholder(tf.float32, [None, n_output], name='target_output')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')  # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensor_map = {x: dataset.PreProcessed.Train.feature, \n",
    "              y: dataset.PreProcessed.Train.target, \n",
    "              keep_prob: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.PreProcessed.Train.feature[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    with tf.name_scope('array_reshape') as array_reshape:\n",
    "        x = tf.reshape(x, shape=[-1, 12, 12, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    with tf.name_scope('conv_layer1') as conv_layer1:\n",
    "        conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    with tf.name_scope('conv_layer2') as conv_layer2:\n",
    "        conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    with tf.name_scope('fc_layer') as fc_layer:\n",
    "        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        # Apply Dropout\n",
    "        fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    with tf.name_scope('output_layer') as output_layer:\n",
    "        out = tf.nn.sigmoid(tf.add(tf.matmul(fc1, weights['out']), biases['out']))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 3x3 conv, 1 input, 32 outputs(number of filter = 32)\n",
    "    'wc1': tf.Variable(tf.truncated_normal([3, 3, 1, 32]), name='wc1'),\n",
    "    \n",
    "    # 3x3 conv, 32 inputs, 64 outputs(number of filter = 64)\n",
    "    'wc2': tf.Variable(tf.truncated_normal([3, 3, 32, 64]), name='wc2'),\n",
    "    \n",
    "    # fully connected, width*height*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.truncated_normal([3 * 3 * 64, 64*9]), name='wd1'),\n",
    "    \n",
    "    # 1024 inputs, 48 outputs\n",
    "    'out': tf.Variable(tf.truncated_normal([64*9, n_output]), name='wo1')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.truncated_normal([32]), name='bc1'),\n",
    "    'bc2': tf.Variable(tf.truncated_normal([64]), name='bc2'),\n",
    "    'bd1': tf.Variable(tf.truncated_normal([64*9]), name='bd1'),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_output]), name='bo1')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope('cost'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    tf.summary.scalar('cost/', cost)\n",
    "    \n",
    "with tf.name_scope('optimization'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluation model\n",
    "with tf.name_scope('evaluation'):\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y, pred))))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# next batch data selector\n",
    "def next_batch(dataset, batch_size, step):\n",
    "    \"\"\"\n",
    "    pick data and build next batch for training\n",
    "    :dataset\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    start_idx = (step - 1) * batch_size\n",
    "    end_idx = step * batch_size\n",
    "    \n",
    "    index_checker = end_idx - len(dataset)\n",
    "    \n",
    "    if index_checker < 0:\n",
    "        batch_data = dataset[start_idx : end_idx]\n",
    "    else:\n",
    "        batch_data = np.concatenate((dataset[start_idx : len(dataset)], dataset[0 : index_checker]), axis=0)\n",
    "        \n",
    "    if len(batch_data) == batch_size:\n",
    "        return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2500 cost : 109.265778 rmse : 0.503096\n",
      "Iter 5000 cost : 108.769264 rmse : 0.487693\n",
      "Iter 7500 cost : 108.697159 rmse : 0.494173\n",
      "Iter 10000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 12500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 15000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 17500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 20000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 22500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 25000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 27500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 30000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 32500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 35000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 37500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 40000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 42500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 45000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 47500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 50000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 52500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 55000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 57500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 60000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 62500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 65000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 67500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 70000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 72500 cost : 108.321022 rmse : 0.473012\n",
      "Iter 75000 cost : 108.321022 rmse : 0.473012\n",
      "Iter 77500 cost : 108.321022 rmse : 0.473012\n",
      "Optimization Finished!\n",
      "('Testing Accuracy [RMSE] :', 0.46584618)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('./summary', sess.graph)\n",
    "    \n",
    "    step = 1\n",
    "    \n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "#         batch_x = next_batch(temperature_map, batch_size, step)\n",
    "#         batch_y = next_batch(power_vector_, batch_size, step)\n",
    "\n",
    "        batch_x = dataset.PreProcessed.Train.feature\n",
    "        batch_y = dataset.PreProcessed.Train.target\n",
    "        \n",
    "    \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.75})\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "            acc = sess.run(rmse, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "            \n",
    "            print \"Iter \" + str(step),\n",
    "            print \"cost : \" + \"{:.6f}\".format(loss),\n",
    "            print \"rmse : \" + \"{:.6f}\".format(acc)\n",
    "            #print(\"Iter \" + str(step * batch_size) + \", Minibatch Loss= \" + \\\n",
    "            #     \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "            #    \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    print(\"Testing Accuracy [RMSE] :\", \\\n",
    "        sess.run(rmse, feed_dict={x: dataset.PreProcessed.Test.feature,\n",
    "                                  y: dataset.PreProcessed.Test.target,\n",
    "                                  keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model load -> predict -> draw graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
