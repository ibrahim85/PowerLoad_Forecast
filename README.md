# PowerLoad_Forecast

###[Abstract]

Since the concept of machine learning has been devised, there have been efforts to have better performance in various ways. There are two main ways to do this: the first is the improvement of the performance of the learning algorithm, and the second is how well the data features are extracted through the preprocessing process. There are four basic and advnaced learning algoithms: 1) Machine Learning(ML) algorithm which is trained by gradient based algorithm and known as a basic concept of learning algorithm; 2) Deep Learning(DL) algorithm, based on ML algorithm and having more hidden layers allow to learn high-level features; 3) Extreme Learning Machine algorithm            ; and 4) Hi Extreme Learning Machine(H-ELM)           . 
In this paper, we design an algorithm for predicting power usage based on Australian National Electricity Market (NEM) data. It is designed to perform the performance evaluation according to the above four methods of learning algorithm and data feature generation pre-processing. Owing to the very fast training/tuning speed of ELM and multilayer concept, H-ELM show that the training efficiency and the forecasting accuracy are superior over the competitive algorithms.


Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parame- ters are randomly generated and the output weights are analyti- cally computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via l1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state- of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.

