{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print (type(mnist.train.images))\n",
    "print (mnist.train.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.38039219  0.37647063\n",
      "  0.3019608   0.46274513  0.2392157   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.35294119  0.5411765\n",
      "  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869  0.92156869\n",
      "  0.98431379  0.98431379  0.97254908  0.99607849  0.96078438  0.92156869\n",
      "  0.74509805  0.08235294  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.54901963  0.98431379  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.74117649  0.09019608\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.88627458  0.99607849  0.81568635\n",
      "  0.78039223  0.78039223  0.78039223  0.78039223  0.54509807  0.2392157\n",
      "  0.2392157   0.2392157   0.2392157   0.2392157   0.50196081  0.8705883\n",
      "  0.99607849  0.99607849  0.74117649  0.08235294  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14901961  0.32156864  0.0509804   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.13333334  0.83529419  0.99607849  0.99607849  0.45098042  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.32941177  0.99607849  0.99607849  0.91764712  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.41568631  0.6156863   0.99607849  0.99607849  0.95294124  0.20000002\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.09803922  0.45882356  0.89411771\n",
      "  0.89411771  0.89411771  0.99215692  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.94117653  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.26666668  0.4666667   0.86274517\n",
      "  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "  0.99607849  0.99607849  0.99607849  0.55686277  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.14509805  0.73333335  0.99215692\n",
      "  0.99607849  0.99607849  0.99607849  0.87450987  0.80784321  0.80784321\n",
      "  0.29411766  0.26666668  0.84313732  0.99607849  0.99607849  0.45882356\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.44313729\n",
      "  0.8588236   0.99607849  0.94901967  0.89019614  0.45098042  0.34901962\n",
      "  0.12156864  0.          0.          0.          0.          0.7843138\n",
      "  0.99607849  0.9450981   0.16078432  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.66274512  0.99607849  0.6901961   0.24313727  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.18823531\n",
      "  0.90588242  0.99607849  0.91764712  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.07058824  0.48627454  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.32941177  0.99607849  0.99607849  0.65098041  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.54509807  0.99607849  0.9333334   0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.82352948  0.98039222  0.99607849  0.65882355  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.94901967  0.99607849  0.93725497  0.22352943  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.34901962  0.98431379  0.9450981   0.33725491  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01960784  0.80784321  0.96470594  0.6156863   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.01568628  0.45882356  0.27058825  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.        ]\n",
      "<type 'numpy.ndarray'>\n",
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "print (mnist.train.images[0])\n",
    "print (type(mnist.train.images[0]))\n",
    "print (mnist.train.images[0].shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input], name='input_data')\n",
    "y = tf.placeholder(tf.float32, [None, n_classes], name='output_class')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')  # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    with tf.name_scope('array_reshape') as array_reshape:\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    with tf.name_scope('conv_layer1') as conv_layer1:\n",
    "        conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    with tf.name_scope('conv_layer2') as conv_layer2:\n",
    "        conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    with tf.name_scope('fc_layer') as fc_layer:\n",
    "        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        # Apply Dropout\n",
    "        fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    with tf.name_scope('output_layer') as output_layer:\n",
    "        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32]), name='wc1'),\n",
    "    \n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64]), name='wc2'),\n",
    "    \n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7 * 7 * 64, 1024]), name='wd1'),\n",
    "    \n",
    "    # 1024 inputs, 48 outputs\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]), name='wo1')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32]), name='bc1'),\n",
    "    'bc2': tf.Variable(tf.random_normal([64]), name='bc2'),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024]), name='bd1'),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]), name='bo1')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_list = [weights[\"wc1\"], biases[\"bc1\"], \n",
    "              weights[\"wc2\"], biases[\"bc2\"], \n",
    "              weights[\"wd1\"], biases[\"bd1\"], \n",
    "              weights[\"out\"], biases[\"out\"]]\n",
    "saver = tf.train.Saver(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope('cost'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    tf.summary.scalar('cost/', cost)\n",
    "with tf.name_scope('optimization'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "with tf.name_scope('evaluation'):\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    rmse = tf.metrics.root_mean_squared_error(labels=y, predictions=pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280\n",
      "Minibatch Loss= 19064.607422\n",
      "Training Accuracy= 0.32812\n",
      "Iter 2560\n",
      "Minibatch Loss= 11226.175781\n",
      "Training Accuracy= 0.55469\n",
      "Iter 3840\n",
      "Minibatch Loss= 8094.711426\n",
      "Training Accuracy= 0.60156\n",
      "Iter 5120\n",
      "Minibatch Loss= 6091.456543\n",
      "Training Accuracy= 0.69531\n",
      "Iter 6400\n",
      "Minibatch Loss= 3598.299316\n",
      "Training Accuracy= 0.77344\n",
      "Iter 7680\n",
      "Minibatch Loss= 6737.310547\n",
      "Training Accuracy= 0.70312\n",
      "Iter 8960\n",
      "Minibatch Loss= 2905.686523\n",
      "Training Accuracy= 0.78906\n",
      "Iter 10240\n",
      "Minibatch Loss= 3830.239014\n",
      "Training Accuracy= 0.78125\n",
      "Iter 11520\n",
      "Minibatch Loss= 1440.343872\n",
      "Training Accuracy= 0.90625\n",
      "Iter 12800\n",
      "Minibatch Loss= 2730.257324\n",
      "Training Accuracy= 0.83594\n",
      "Iter 14080\n",
      "Minibatch Loss= 1594.744751\n",
      "Training Accuracy= 0.89062\n",
      "Iter 15360\n",
      "Minibatch Loss= 1362.139038\n",
      "Training Accuracy= 0.91406\n",
      "Iter 16640\n",
      "Minibatch Loss= 1820.742310\n",
      "Training Accuracy= 0.90625\n",
      "Iter 17920\n",
      "Minibatch Loss= 1374.810791\n",
      "Training Accuracy= 0.88281\n",
      "Iter 19200\n",
      "Minibatch Loss= 1345.087280\n",
      "Training Accuracy= 0.91406\n",
      "Iter 20480\n",
      "Minibatch Loss= 707.561218\n",
      "Training Accuracy= 0.92188\n",
      "Iter 21760\n",
      "Minibatch Loss= 3579.134766\n",
      "Training Accuracy= 0.85156\n",
      "Iter 23040\n",
      "Minibatch Loss= 905.576172\n",
      "Training Accuracy= 0.94531\n",
      "Iter 24320\n",
      "Minibatch Loss= 1185.559692\n",
      "Training Accuracy= 0.89844\n",
      "Iter 25600\n",
      "Minibatch Loss= 979.361938\n",
      "Training Accuracy= 0.92188\n",
      "Iter 26880\n",
      "Minibatch Loss= 1184.514160\n",
      "Training Accuracy= 0.89062\n",
      "Iter 28160\n",
      "Minibatch Loss= 1112.166260\n",
      "Training Accuracy= 0.93750\n",
      "Iter 29440\n",
      "Minibatch Loss= 1202.222656\n",
      "Training Accuracy= 0.92969\n",
      "Iter 30720\n",
      "Minibatch Loss= 755.549805\n",
      "Training Accuracy= 0.92969\n",
      "Iter 32000\n",
      "Minibatch Loss= 685.320435\n",
      "Training Accuracy= 0.94531\n",
      "Iter 33280\n",
      "Minibatch Loss= 1116.427856\n",
      "Training Accuracy= 0.91406\n",
      "Iter 34560\n",
      "Minibatch Loss= 394.466766\n",
      "Training Accuracy= 0.95312\n",
      "Iter 35840\n",
      "Minibatch Loss= 223.337463\n",
      "Training Accuracy= 0.97656\n",
      "Iter 37120\n",
      "Minibatch Loss= 1237.475342\n",
      "Training Accuracy= 0.92188\n",
      "Iter 38400\n",
      "Minibatch Loss= 62.191818\n",
      "Training Accuracy= 0.97656\n",
      "Iter 39680\n",
      "Minibatch Loss= 140.363388\n",
      "Training Accuracy= 0.97656\n",
      "Iter 40960\n",
      "Minibatch Loss= 1108.544922\n",
      "Training Accuracy= 0.90625\n",
      "Iter 42240\n",
      "Minibatch Loss= 743.550781\n",
      "Training Accuracy= 0.94531\n",
      "Iter 43520\n",
      "Minibatch Loss= 582.461365\n",
      "Training Accuracy= 0.96094\n",
      "Iter 44800\n",
      "Minibatch Loss= 266.793182\n",
      "Training Accuracy= 0.96094\n",
      "Iter 46080\n",
      "Minibatch Loss= 173.240601\n",
      "Training Accuracy= 0.97656\n",
      "Iter 47360\n",
      "Minibatch Loss= 1110.853882\n",
      "Training Accuracy= 0.90625\n",
      "Iter 48640\n",
      "Minibatch Loss= 1398.486572\n",
      "Training Accuracy= 0.92969\n",
      "Iter 49920\n",
      "Minibatch Loss= 413.590759\n",
      "Training Accuracy= 0.96094\n",
      "Iter 51200\n",
      "Minibatch Loss= 232.081451\n",
      "Training Accuracy= 0.96094\n",
      "Iter 52480\n",
      "Minibatch Loss= 182.752548\n",
      "Training Accuracy= 0.96094\n",
      "Iter 53760\n",
      "Minibatch Loss= 137.950226\n",
      "Training Accuracy= 0.96875\n",
      "Iter 55040\n",
      "Minibatch Loss= 235.430054\n",
      "Training Accuracy= 0.96875\n",
      "Iter 56320\n",
      "Minibatch Loss= 213.194122\n",
      "Training Accuracy= 0.96094\n",
      "Iter 57600\n",
      "Minibatch Loss= 759.262024\n",
      "Training Accuracy= 0.90625\n",
      "Iter 58880\n",
      "Minibatch Loss= 934.314331\n",
      "Training Accuracy= 0.92188\n",
      "Iter 60160\n",
      "Minibatch Loss= 369.621735\n",
      "Training Accuracy= 0.95312\n",
      "Iter 61440\n",
      "Minibatch Loss= 429.692474\n",
      "Training Accuracy= 0.94531\n",
      "Iter 62720\n",
      "Minibatch Loss= 290.985809\n",
      "Training Accuracy= 0.95312\n",
      "Iter 64000\n",
      "Minibatch Loss= 361.072968\n",
      "Training Accuracy= 0.96875\n",
      "Iter 65280\n",
      "Minibatch Loss= 413.757996\n",
      "Training Accuracy= 0.97656\n",
      "Iter 66560\n",
      "Minibatch Loss= 933.896790\n",
      "Training Accuracy= 0.94531\n",
      "Iter 67840\n",
      "Minibatch Loss= 1003.330444\n",
      "Training Accuracy= 0.92969\n",
      "Iter 69120\n",
      "Minibatch Loss= 283.722107\n",
      "Training Accuracy= 0.97656\n",
      "Iter 70400\n",
      "Minibatch Loss= 936.381714\n",
      "Training Accuracy= 0.93750\n",
      "Iter 71680\n",
      "Minibatch Loss= 680.267151\n",
      "Training Accuracy= 0.94531\n",
      "Iter 72960\n",
      "Minibatch Loss= 403.385254\n",
      "Training Accuracy= 0.94531\n",
      "Iter 74240\n",
      "Minibatch Loss= 390.246429\n",
      "Training Accuracy= 0.95312\n",
      "Iter 75520\n",
      "Minibatch Loss= 950.149658\n",
      "Training Accuracy= 0.96094\n",
      "Iter 76800\n",
      "Minibatch Loss= 346.694336\n",
      "Training Accuracy= 0.95312\n",
      "Iter 78080\n",
      "Minibatch Loss= 349.738342\n",
      "Training Accuracy= 0.95312\n",
      "Iter 79360\n",
      "Minibatch Loss= 48.377228\n",
      "Training Accuracy= 0.98438\n",
      "Iter 80640\n",
      "Minibatch Loss= 427.338440\n",
      "Training Accuracy= 0.96094\n",
      "Iter 81920\n",
      "Minibatch Loss= 820.229309\n",
      "Training Accuracy= 0.93750\n",
      "Iter 83200\n",
      "Minibatch Loss= 281.654755\n",
      "Training Accuracy= 0.96875\n",
      "Iter 84480\n",
      "Minibatch Loss= 219.393280\n",
      "Training Accuracy= 0.96875\n",
      "Iter 85760\n",
      "Minibatch Loss= 233.769150\n",
      "Training Accuracy= 0.97656\n",
      "Iter 87040\n",
      "Minibatch Loss= 775.332947\n",
      "Training Accuracy= 0.96094\n",
      "Iter 88320\n",
      "Minibatch Loss= 637.887634\n",
      "Training Accuracy= 0.93750\n",
      "Iter 89600\n",
      "Minibatch Loss= 664.128174\n",
      "Training Accuracy= 0.95312\n",
      "Iter 90880\n",
      "Minibatch Loss= 62.660988\n",
      "Training Accuracy= 0.98438\n",
      "Iter 92160\n",
      "Minibatch Loss= 198.250443\n",
      "Training Accuracy= 0.96875\n",
      "Iter 93440\n",
      "Minibatch Loss= 279.313538\n",
      "Training Accuracy= 0.96094\n",
      "Iter 94720\n",
      "Minibatch Loss= 109.111221\n",
      "Training Accuracy= 0.96875\n",
      "Iter 96000\n",
      "Minibatch Loss= 227.544037\n",
      "Training Accuracy= 0.99219\n",
      "Iter 97280\n",
      "Minibatch Loss= 419.468689\n",
      "Training Accuracy= 0.96094\n",
      "Iter 98560\n",
      "Minibatch Loss= 0.000000\n",
      "Training Accuracy= 1.00000\n",
      "Iter 99840\n",
      "Minibatch Loss= 390.375916\n",
      "Training Accuracy= 0.95312\n",
      "Iter 101120\n",
      "Minibatch Loss= 266.075867\n",
      "Training Accuracy= 0.96875\n",
      "Iter 102400\n",
      "Minibatch Loss= 658.727173\n",
      "Training Accuracy= 0.94531\n",
      "Iter 103680\n",
      "Minibatch Loss= 191.325378\n",
      "Training Accuracy= 0.94531\n",
      "Iter 104960\n",
      "Minibatch Loss= 333.880615\n",
      "Training Accuracy= 0.93750\n",
      "Iter 106240\n",
      "Minibatch Loss= 178.862488\n",
      "Training Accuracy= 0.96875\n",
      "Iter 107520\n",
      "Minibatch Loss= 123.472557\n",
      "Training Accuracy= 0.96094\n",
      "Iter 108800\n",
      "Minibatch Loss= 356.564697\n",
      "Training Accuracy= 0.96094\n",
      "Iter 110080\n",
      "Minibatch Loss= 107.687325\n",
      "Training Accuracy= 0.98438\n",
      "Iter 111360\n",
      "Minibatch Loss= 249.315369\n",
      "Training Accuracy= 0.97656\n",
      "Iter 112640\n",
      "Minibatch Loss= 304.051086\n",
      "Training Accuracy= 0.96875\n",
      "Iter 113920\n",
      "Minibatch Loss= 218.149155\n",
      "Training Accuracy= 0.96875\n",
      "Iter 115200\n",
      "Minibatch Loss= 411.781769\n",
      "Training Accuracy= 0.93750\n",
      "Iter 116480\n",
      "Minibatch Loss= 203.519775\n",
      "Training Accuracy= 0.98438\n",
      "Iter 117760\n",
      "Minibatch Loss= 247.517075\n",
      "Training Accuracy= 0.96875\n",
      "Iter 119040\n",
      "Minibatch Loss= 70.782104\n",
      "Training Accuracy= 0.97656\n",
      "Iter 120320\n",
      "Minibatch Loss= 132.171280\n",
      "Training Accuracy= 0.96875\n",
      "Iter 121600\n",
      "Minibatch Loss= 243.705444\n",
      "Training Accuracy= 0.97656\n",
      "Iter 122880\n",
      "Minibatch Loss= 140.583130\n",
      "Training Accuracy= 0.96094\n",
      "Iter 124160\n",
      "Minibatch Loss= 167.403564\n",
      "Training Accuracy= 0.97656\n",
      "Iter 125440\n",
      "Minibatch Loss= 700.985596\n",
      "Training Accuracy= 0.96094\n",
      "Iter 126720\n",
      "Minibatch Loss= 33.235710\n",
      "Training Accuracy= 0.97656\n",
      "Iter 128000\n",
      "Minibatch Loss= 300.316406\n",
      "Training Accuracy= 0.96875\n",
      "Iter 129280\n",
      "Minibatch Loss= 75.576920\n",
      "Training Accuracy= 0.99219\n",
      "Iter 130560\n",
      "Minibatch Loss= 247.037643\n",
      "Training Accuracy= 0.96875\n",
      "Iter 131840\n",
      "Minibatch Loss= 401.371704\n",
      "Training Accuracy= 0.93750\n",
      "Iter 133120\n",
      "Minibatch Loss= 9.910736\n",
      "Training Accuracy= 0.99219\n",
      "Iter 134400\n",
      "Minibatch Loss= 91.259392\n",
      "Training Accuracy= 0.97656\n",
      "Iter 135680\n",
      "Minibatch Loss= 238.326569\n",
      "Training Accuracy= 0.96875\n",
      "Iter 136960\n",
      "Minibatch Loss= 46.797966\n",
      "Training Accuracy= 0.97656\n",
      "Iter 138240\n",
      "Minibatch Loss= 157.711304\n",
      "Training Accuracy= 0.98438\n",
      "Iter 139520\n",
      "Minibatch Loss= 612.826233\n",
      "Training Accuracy= 0.95312\n",
      "Iter 140800\n",
      "Minibatch Loss= 363.736237\n",
      "Training Accuracy= 0.96875\n",
      "Iter 142080\n",
      "Minibatch Loss= 231.887268\n",
      "Training Accuracy= 0.97656\n",
      "Iter 143360\n",
      "Minibatch Loss= 31.440033\n",
      "Training Accuracy= 0.99219\n",
      "Iter 144640\n",
      "Minibatch Loss= 18.913116\n",
      "Training Accuracy= 0.97656\n",
      "Iter 145920\n",
      "Minibatch Loss= 86.364182\n",
      "Training Accuracy= 0.96094\n",
      "Iter 147200\n",
      "Minibatch Loss= 355.892365\n",
      "Training Accuracy= 0.96094\n",
      "Iter 148480\n",
      "Minibatch Loss= 123.119278\n",
      "Training Accuracy= 0.98438\n",
      "Iter 149760\n",
      "Minibatch Loss= 300.327515\n",
      "Training Accuracy= 0.96094\n",
      "Iter 151040\n",
      "Minibatch Loss= 176.736267\n",
      "Training Accuracy= 0.96875\n",
      "Iter 152320\n",
      "Minibatch Loss= 284.739105\n",
      "Training Accuracy= 0.93750\n",
      "Iter 153600\n",
      "Minibatch Loss= 160.599213\n",
      "Training Accuracy= 0.98438\n",
      "Iter 154880\n",
      "Minibatch Loss= 239.316574\n",
      "Training Accuracy= 0.95312\n",
      "Iter 156160\n",
      "Minibatch Loss= 317.903717\n",
      "Training Accuracy= 0.97656\n",
      "Iter 157440\n",
      "Minibatch Loss= 224.584229\n",
      "Training Accuracy= 0.97656\n",
      "Iter 158720\n",
      "Minibatch Loss= 147.914642\n",
      "Training Accuracy= 0.96875\n",
      "Iter 160000\n",
      "Minibatch Loss= 240.507935\n",
      "Training Accuracy= 0.96875\n",
      "Iter 161280\n",
      "Minibatch Loss= 275.836914\n",
      "Training Accuracy= 0.96094\n",
      "Iter 162560\n",
      "Minibatch Loss= 84.348465\n",
      "Training Accuracy= 0.96875\n",
      "Iter 163840\n",
      "Minibatch Loss= 476.398590\n",
      "Training Accuracy= 0.96875\n",
      "Iter 165120\n",
      "Minibatch Loss= 130.433365\n",
      "Training Accuracy= 0.95312\n",
      "Iter 166400\n",
      "Minibatch Loss= 114.011337\n",
      "Training Accuracy= 0.98438\n",
      "Iter 167680\n",
      "Minibatch Loss= 8.983719\n",
      "Training Accuracy= 0.99219\n",
      "Iter 168960\n",
      "Minibatch Loss= 159.026108\n",
      "Training Accuracy= 0.96875\n",
      "Iter 170240\n",
      "Minibatch Loss= 141.771927\n",
      "Training Accuracy= 0.96094\n",
      "Iter 171520\n",
      "Minibatch Loss= 4.565765\n",
      "Training Accuracy= 0.99219\n",
      "Iter 172800\n",
      "Minibatch Loss= 61.324181\n",
      "Training Accuracy= 0.97656\n",
      "Iter 174080\n",
      "Minibatch Loss= 185.085388\n",
      "Training Accuracy= 0.98438\n",
      "Iter 175360\n",
      "Minibatch Loss= 269.824890\n",
      "Training Accuracy= 0.96094\n",
      "Iter 176640\n",
      "Minibatch Loss= 206.848587\n",
      "Training Accuracy= 0.97656\n",
      "Iter 177920\n",
      "Minibatch Loss= 73.935043\n",
      "Training Accuracy= 0.97656\n",
      "Iter 179200\n",
      "Minibatch Loss= 62.173935\n",
      "Training Accuracy= 0.97656\n",
      "Iter 180480\n",
      "Minibatch Loss= 119.647552\n",
      "Training Accuracy= 0.97656\n",
      "Iter 181760\n",
      "Minibatch Loss= 0.000000\n",
      "Training Accuracy= 1.00000\n",
      "Iter 183040\n",
      "Minibatch Loss= 319.059937\n",
      "Training Accuracy= 0.95312\n",
      "Iter 184320\n",
      "Minibatch Loss= 47.264992\n",
      "Training Accuracy= 0.99219\n",
      "Iter 185600\n",
      "Minibatch Loss= 137.758698\n",
      "Training Accuracy= 0.98438\n",
      "Iter 186880\n",
      "Minibatch Loss= 114.903641\n",
      "Training Accuracy= 0.97656\n",
      "Iter 188160\n",
      "Minibatch Loss= 154.914917\n",
      "Training Accuracy= 0.97656\n",
      "Iter 189440\n",
      "Minibatch Loss= 199.842987\n",
      "Training Accuracy= 0.96094\n",
      "Iter 190720\n",
      "Minibatch Loss= 174.937943\n",
      "Training Accuracy= 0.98438\n",
      "Iter 192000\n",
      "Minibatch Loss= 383.309692\n",
      "Training Accuracy= 0.96094\n",
      "Iter 193280\n",
      "Minibatch Loss= 137.656311\n",
      "Training Accuracy= 0.99219\n",
      "Iter 194560\n",
      "Minibatch Loss= 87.353806\n",
      "Training Accuracy= 0.98438\n",
      "Iter 195840\n",
      "Minibatch Loss= 61.021393\n",
      "Training Accuracy= 0.98438\n",
      "Iter 197120\n",
      "Minibatch Loss= 98.152863\n",
      "Training Accuracy= 0.99219\n",
      "Iter 198400\n",
      "Minibatch Loss= 102.622459\n",
      "Training Accuracy= 0.97656\n",
      "Iter 199680\n",
      "Minibatch Loss= 114.907776\n",
      "Training Accuracy= 0.98438\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.976562\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('./summary', sess.graph)  \n",
    "    \n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        summary, _ = sess.run([merged, optimizer], \n",
    "                              feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        \n",
    "        train_writer.add_summary(summary, step * batch_size)\n",
    "        saver.save(sess, './tensorflow.ckpt')\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], \n",
    "                                      feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step * batch_size))\n",
    "            print (\"Minibatch Loss= \" + \"{:.6f}\".format(loss))\n",
    "            print (\"Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "#             print (\"RMSE= \" + \"{:.5f}\".format(acc))\n",
    "            print\n",
    "        step += 1\n",
    "\n",
    "#     for i in xrange(0,20000):\n",
    "#         loss, acc, err = sess.run([cost, accuracy, rmse],\n",
    "#                                   feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "#         if i/100 == 0:\n",
    "#             print (\"iter= \" + i)\n",
    "#             print (\"loss= \" + \"{:.6f}\".format(loss))\n",
    "#             print (\"acc= \" + \"{:.6f}\".format(lacc))\n",
    "#             print (\"rmse= \" + \"{:.6f}\".format(err))\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(thread)\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\",\n",
    "          sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                        y: mnist.test.labels[:256],\n",
    "                                        keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = mnist.train.next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "<type 'numpy.ndarray'>\n",
      "(128, 784)\n"
     ]
    }
   ],
   "source": [
    "print (batch_x)\n",
    "print (type(batch_x))\n",
    "print (batch_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
